{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping With Python\n",
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Whether you are a data scientist, engineer, or anybody who analyzes large amounts of datasets, the ability to scrape data from the web is a useful skill to have. Let's say you find data from the web, and there is no direct way to download it, web scraping using Python is a skill you can use to extract the data into a useful form that can be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.hubertiming.com/results/2017GPTR10K\"\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beautiful Soup package is used to parse the html, that is, take the raw html text and break it into Python objects. The second argument 'lxml' is the html parser. The soup object allows you to extract interesting information about the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>2017 Intel Great Place to Run 10K \\ Urban Clash Games Race Results</title>\n"
     ]
    }
   ],
   "source": [
    "# Get the title\n",
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the page as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output file\n",
    "file_name = f'{url.split(\"/\")[-1]}.txt'\n",
    "with open(str(file_name), 'w', encoding='utf-8') as f_out:\n",
    "    f_out.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the find_all() method of soup to extract useful html tags within a webpage. Examples of useful tags include < a > for hyperlinks, < table > for tables, < tr > for table rows, < th > for table headers, and < td > for table cells. The code below shows how to extract all the hyperlinks within the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"mailto:timing@hubertiming.com\">timing@hubertiming.com</a>,\n",
       " <a href=\"https://www.hubertiming.com/\">Huber Timing Home</a>,\n",
       " <a class=\"btn btn-primary btn-lg\" href=\"/results/2017GPTR\" role=\"button\" style=\"margin: 0px 0px 5px 5px\">5K</a>,\n",
       " <a class=\"btn btn-primary btn-lg\" href=\"/results/summary/2017GPTR10K\" role=\"button\" style=\"margin: 0px 0px 5px 5px\">Summary</a>,\n",
       " <a class=\"btn btn-secondary btn-sm\" href=\"#team\" role=\"button\"><i aria-hidden=\"true\" class=\"fa fa-users\"></i> Team Results</a>,\n",
       " <a class=\"btn btn-secondary btn-sm\" href=\"#individual\" role=\"button\"><i aria-hidden=\"true\" class=\"fa fa-user\"></i> Individual Results</a>,\n",
       " <a name=\"team\"></a>,\n",
       " <a id=\"individual\" name=\"individual\"></a>,\n",
       " <a href=\"#tabs-1\" style=\"font-size: 18px\">10K Results</a>,\n",
       " <a href=\"https://www.hubertiming.com/\"><img height=\"65\" src=\"/sites/all/themes/hubertiming/images/clockWithFinishSign_small.png\" width=\"50\"/>Huber Timing</a>,\n",
       " <a href=\"https://facebook.com/hubertiming/\"><img src=\"/results/FB-f-Logo__blue_50.png\"/></a>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the links of the page\n",
    "As you can see from the output above, html tags sometimes come with attributes such as class, src, etc. These attributes provide additional information about html elements. You can use a for loop and the get('\"href\") method to extract and print out only hyperlinks.\n",
    "\n",
    "We can group the links like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mailto:timing@hubertiming.com']\n",
      "['https://www.hubertiming.com/', 'https://www.hubertiming.com/', 'https://facebook.com/hubertiming/']\n",
      "['http://www.hubertiming.com//results/2017GPTR', 'http://www.hubertiming.com//results/summary/2017GPTR10K', 'http://www.hubertiming.com/results/2017GPTR10K#team', 'http://www.hubertiming.com/results/2017GPTR10K#individual', 'http://www.hubertiming.com/results/2017GPTR10K#tabs-1']\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlsplit\n",
    "base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "#Grouping all the links\n",
    "mail_address = []\n",
    "external_link = []\n",
    "internal_link = []\n",
    "all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    link = link.get(\"href\")\n",
    "    if link:\n",
    "        if \"mailto\" in link:\n",
    "            mail_address.append(link)\n",
    "        elif \"www\" in link or \".com\" in link:\n",
    "            external_link.append(link)\n",
    "        elif link.startswith('#'):\n",
    "            link = url+link\n",
    "            internal_link.append(link)\n",
    "        else:\n",
    "            link = base_url + link\n",
    "            internal_link.append(link)\n",
    "\n",
    "print(mail_address)\n",
    "print(external_link)\n",
    "print(internal_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Table\n",
    "To print out table rows only, pass the 'tr' argument in soup.find_all()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tr colspan=\"2\"><b>10K:</b></tr>, <tr><td>Finishers:</td><td>577</td></tr>, <tr><td>Male:</td><td>414</td></tr>, <tr><td>Female:</td><td>163</td></tr>, <tr>\n",
      "<td>Award</td>\n",
      "<td>Name</td>\n",
      "<td>Combined Time</td>\n",
      "<td>1</td><td>2</td><td>3</td><td>4</td></tr>]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows for sanity check\n",
    "rows = soup.find_all('tr')\n",
    "print(rows[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.hubertiming.com/results/2017GPTR10K\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html)\n",
    "table = soup.find(\"table\", id='individualResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "output_rows = []\n",
    "table_header = []\n",
    "#Get the Header of the table\n",
    "for table_row in table.findAll('th'):\n",
    "    table_header.append(table_row.get_text())\n",
    "#Get the rows of the table\n",
    "for table_row in table.findAll('tr'):\n",
    "    columns = table_row.findAll('td')\n",
    "    output_row = []\n",
    "    for column in columns:\n",
    "        output_row.append(column.text)\n",
    "    output_rows.append(output_row)\n",
    "#Save the table as CSV\n",
    "with open('output.csv', 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(table_header)\n",
    "    writer.writerows(output_rows[1:])#The first row is of header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
